= brr-base-tf-resources
:toc:

[NOTE]
This is not the cleanest repo I have ever written so there may be some visible kruft around.

[TIP]
Look at the branches of this project. One is configured for Terraform, another is configured for OpenTofu and there are slight variations between the two

The goal of this repo is to provide a complete working template for new cloud account setups including base CI jobs to facilitate clear and easy setup and configuration.

Eventually I hope to have templates for multiple cloud providers. As it currently stands all projects are written with Aws in mind and are contained under the aws folder.

== The various ways to deploy EKS

The `002-setup-eks` project contains multiple valid ways in which to set up and configure your eks cluster. However, these various examples are in varying states of working order. Minimally all projects will successfully deploy an EKS cluster to Aws. See details below for state ...

* `001`
** Fully working as described in read me
** Can deploy argo
* `002`
** Works like project `001`
** Pod identity is not fully configured
* `003`
** Auto mode is enabled
** No node groups are created
** So far all deployments have failed taking to long
** Authentication is the same as project `001`
* `004`
** Auto mode is enabled
** No node groups are created
** So far all deployments have failed taking to long
** Pod identity is not fully configured
** Authentication is the same as project `001`
* `005`
** Uses HashiCorp provided EKS Module
** Deploys cluster but fails to create node groups
** DOES NOT CURRENTLY WORK
* `006`
** Uses HashiCorp provided EKS Module
** Deploys cluster but fails to create node groups
** DOES NOT CURRENTLY WORK

[NOTE]
Please reference the sub-readmes in each of the specified projects.

== Aws

=== Directory structure

This project could _technically_ be vastly simplified and all Terraform/OpenTofu projects could be placed directly under the `aws` folder. This is due to the fact that all these projects are configured in such a way that account and environments can be passed into them and executed idempotently across accounts and regions.

However, this becomes an issue when executing locally due to how Terraform/OpenTofu maintains its state. In example if all the projects are just under `aws` and I want to execute them from my machine to both dev and prod; whatever the second execution is will try to overwrite the first executions state. For this reason Terraform/OpenTofu projects must be duplicated per account and or region.

[NOTE]
Some of this can be remediated by only executing in CI and using OpenTofu instead of Terraform as OpenTofu allows for variables in the `terraform` block, meaning that the path to where the state is stored can be changed programmatically.

== Azure

- TODO

=== Directory structure

== GCP

- TODO

=== Directory structure

== Deployment Instructions

The following is the order in which projects need to be deployed in order to work correctly. All projects _can_ be deployed/destroyed manually from your local machine using the following commands or a slight variant ...

[CAUTION]
Not all projects require a `var-file` but when one is used it is also required to be used in the destroy command.

.Deploy
[source, shell]
----
terraform init
terraform fmt
terraform validate
terraform apply -auto-approve -var-file=us-east-2.tfvars
----

.Destroy
[source, shell]
----
terraform destroy -auto-approve -var-file=us-east-2.tfvars
----

However, please be aware some projects expect to be deployed manually while others expect to be deployed via CI process. This project is unique in that it assumes you are setting up a cloud account from scratch so there are a few things that you would not normally see or use in here except in that case.

[NOTE]
The project name / number will match the GitHub workflow yaml titles. Please be aware, there is some variation in how workflows work, being that this is a personal project not all workflow operations have been extended to all files.

=== 001-once-per-account / 001-setup-oidc-provider

When setting up a new account this is the first project that needs to be executed if you are using GitHub. This will set up the required Identity Provider within Aws to facilitate the connection. As stated in the title this should be executed once per *ACCOUNT* and has no prerequisites. This also means that it should not be necessary to supply a variables file to this project as this is a global setup.

==== Summary

* Creates an open id connect provider

==== Deploy

[source, shell]
----
terraform init
terraform fmt
terraform validate
terraform apply -auto-approve
----

Being the very first step this project *MUST* be executed manually and is not configured with a CI job. After this job is executed the state *MUST* be migrated for safe keeping to an S3 bucket but this can not be completed until _after_ step [002-once-per-region / 001-setup-s3-bucket]

==== Destroy

[source, shell]
----
 terraform destroy -auto-approve
----

=== 001-once-per-account / 002-setup-iam-roles

When setting up a new account you will require several IAM roles in order to execute with your Cloud account. This project will create a CI role for GH Workflow execution and an admin role such that if required a true IAM role can be assumed from your SSO user.

[NOTE]
SSO users are not true IAM roles and have some limitations within Aws that may or may not cause issues depending on your desired actions.

This project has an additional provider for GitHub so that it can create GitHub secrets. This project will set the created CI role arn as a GH secret as well as the expected Aws region and the allowed execution duration of the role. As stated in the title this should be executed once per *ACCOUNT* and has no prerequisites. While this project is a global setup, it is necessary to supply an arguments file in order to supply the GitHub token to the project so that the GH secrets can be created (region does not matter).

==== Summary

* Creates two IAM roles
** admin_assume_role
** github_oidc_ci_assume_role
* Three GitHub action secrets
** Stores github_oidc_ci_assume_role arn
** Allowed duration of role session
** Aws execution region

==== Deploy

[source, shell]
----
terraform init
terraform fmt
terraform validate
terraform apply -auto-approve -var-file=us-east-2.tfvars
----

Being only the second step, this project still *MUST* be executed manually and is not configured with a CI job. After this job is executed the state *MUST* be migrated for safe keeping to an S3 bucket but this can not be completed until _after_ step [002-once-per-region / 001-setup-s3-bucket]

==== Destroy

[source, shell]
----
 terraform destroy -auto-approve -var-file=us-east-2.tfvars
----

=== 002-once-per-region / 001-setup-s3-bucket

This step is probably the most unique in the deployment process. Technically, this step is not limited to a single execution per region but for the intent of this project a single bucket to hold state per region is all that is required. There are also additional steps and several warnings that come after the initial execution of this project. Considering that no S3 bucket is yet configured the initial execution of this project must happen manually. Meaning that for the time being you have introduced state on your local machine. *DO NOT RUN THIS AGAIN FOR A DIFFERENT REGION FROM YOUR LOCAL MACHINE YET! YOU WILL OVERWRITE YOUR CURRENT STATE AND CAUSE ISSUES!*

This project will create a versioned S3 bucket and will create an additional GH secret value that contains the unique bucket postfix for use future project executions. This is done so that all executions in a given regional stack will have the same value as a reference. This ensures if you have to manually go in and edit or clean up resources in Aws it gives you peace of mind that you are not working on something from a different deployment.

Like the projects before for the initial execution you will perform it manually from your local machine. You must provide a variables file to set the region that the bucket is being created in and to pass in the GH token so that the postfix value can be stored in GitHub

==== Summary

* Can be deployed many times with different options in region *BUT* for our purposes should only be deployed once
* Can be deployed to many regions
** Given appropriate care is taken around how state is stored, otherwise you will accidentally overwrite the state and cause problems
* Creates an S3 bucket to hold Terraform state objects
* Creates GitHub action secret that holds the unique postfix attached to the S3 bucket so this value can be carried throughout the deployment process to help with tracking.

==== Deploy

[source, shell]
----
terraform init
terraform fmt
terraform validate
terraform apply -auto-approve -var-file=us-east-2.tfvars
----

Now, with a S3 bucket finally in place all 3 `tfstate` files from the last 3 steps must be migrated and stored safely in the new bucket. This is especially important for this step `001-setup-s3-bucket` as in order to set up a state bucket in an alternate region you will need to delete your local state and run the project again with a different variables file, and it is super important to not lose the existing state otherwise you will not be able to edit or delete the project resources in the future.

==== Destroy

[source, shell]
----
 terraform destroy -auto-approve -var-file=us-east-2.tfvars
----

=== Pause - MAKE SURE ALL PREVIOUS TFSTATE FILES ARE MIGRATED TO S3

- uncomment s3 backend
- migrate state

Now, with the odd ball setups in place and the tfstate stored securely we can move on to the higher level resources that can be deployed with the GH Workflows using the CI process.

=== Static Analysis

Please be aware that this entire repo is configured with a GH Workflow that runs on push that lints the Terraform in this project. This job runs TFLint, Trivy (TFSec), and Checkov. This runs on every push for all projects and all other project execution should be dependent on this Workflow passing.

Stated another way, the following deployment jobs that us CI are configured to prevent execution if this step does not pass.

=== 003-once-to-many-per-region / 001-setup-vpc

This workflow pair is probably the most built out and demonstrates a number of additional configuration that are not currently built into all the other jobs.

The deploy job verifies that the linting results passed and that the input from the user executing the job is valid prior to creating the vpc(s).

Additionally, there is an environment `all` option which will allow you to deploy to all configured regions for a given environment. I.e. if dev is configured to deploy to us-east-2 and us-west-2 you can select the `dev / all` option and deploy to both at the same time.

While the `all` option is semi convenient for me, I am unsure if this is or would be a good idea for a production level setup. This option required a fair amount of thinking and introduced a number of edge cases and hard coded values into the GH Workflow that would otherwise have not existed. I am not confident that the extra work and complexity is worth the convenience of having the option available. Especially, considering that deploying to multiple environments at the same time in a work / production system will be an exceptionally rare occurrence.

Good or bad I have created the example here for reference.

==== Summary

* Can be deployed many times with different options in region
* Can be deployed to many regions
* Validates input fields
* Verifies project linting prior to execution
* Deploys VPC

[NOTE]
While this project can be executed locally it is preferable to only execute this code using the provided CI tooling.

==== Deploy

* Execute the `003-001-deploy-vpc.yml`
* Navigate to project in GitHub > Actions > Left side select > 003-001 Deploy VPC
* Right hand side > Run workflow dropdown > select options > Run Workflow

==== Destroy

* Execute the `003-001-destroy-vpc.yml`
* Navigate to project in GitHub > Actions > Left side select > 003-001 Destroy VPC
* Right hand side > Run workflow dropdown > select options > Run Workflow
** Make sure the correct name and id are provided in order to remove the correct deployment

[NOTE]
The destroy workflow does not check the linting results as there is no reason for it to care. It will just go through and destroy the necessary resources.

=== 003-once-to-many-per-region / 002-setup-eks

[WARNING]
There are multiple variants of how to get this working, not all of these are 100% working. Please see the individual README's for details.

==== Summary

* This workflow *DEPENDS* on the VPC job having executed successfully
* Can be deployed many times with different options in region
* Can be deployed to many regions
* Validates input fields
* Verifies project linting prior to execution
* Deploys EKS Cluster

[NOTE]
I have identified a total of 8 different variants of how an EKS Cluster can be deployed using Terraform/OpenTofu and have created GH Workflows for 6 of those variants (the 7th in the repo has significant issues currently). All 6 of the GH Workflow variants will at minimum bring an EKS Cluster up successfully. However, the node group attachment success varies by project and is not successful in all cases. Please review the independent project README's for more details

[NOTE]
While this project can be executed locally it is preferable to only execute this code using the provided CI tooling.

==== Deploy

* Execute the `003-002-XXX-deploy-eks.yml`
* Navigate to project in GitHub > Actions > Left side select > 003-002-XXX Deploy EKS
* Right hand side > Run workflow dropdown > select options > Run Workflow

==== Destroy

* Execute the `003-002-XXX-destroy-eks.yml`
* Navigate to project in GitHub > Actions > Left side select > 003-002-XXX Destroy EKS
* Right hand side > Run workflow dropdown > select options > Run Workflow
** Make sure the correct name and id are provided in order to remove the correct deployment

[NOTE]
The destroy workflow does not check the linting results as there is no reason for it to care. It will just go through and destroy the necessary resources.

=== 003-once-to-many-per-region / 003-setup-kubernetes / 001-nginx

==== Summary

* This workflow *DEPENDS* on the VPC and EKS job having executed successfully
* Can be deployed many times with different options in region
* Can be deployed to many regions
* Validates input fields
* Verifies project linting prior to execution
* Deploys Nginx Kubernetes container

[NOTE]
While this project can be executed locally it is preferable to only execute this code using the provided CI tooling.

==== Deploy

* Execute the `003-003-001-deploy-nginx.yml`
* Navigate to project in GitHub > Actions > Left side select > 003-003-001 Deploy Nginx
* Right hand side > Run workflow dropdown > select options > Run Workflow

==== Destroy

* Execute the `003-003-001-destroy-nginx.yml`
* Navigate to project in GitHub > Actions > Left side select > 003-003-001 Destroy Nginx
* Right hand side > Run workflow dropdown > select options > Run Workflow
** Make sure the correct name and id are provided in order to remove the correct deployment

[NOTE]
The destroy workflow does not check the linting results as there is no reason for it to care. It will just go through and destroy the necessary resources.

=== 003-once-to-many-per-region / 003-setup-kubernetes / 002-argo

==== Summary

* This workflow *DEPENDS* on the VPC and EKS job having executed successfully
* Can be deployed many times with different options in region
* Can be deployed to many regions
* Validates input fields
* Verifies project linting prior to execution
* Deploys ArgoCD Kubernetes container

[NOTE]
While this project can be executed locally it is preferable to only execute this code using the provided CI tooling.

==== Deploy

* Execute the `003-003-002-deploy-argo.yml`
* Navigate to project in GitHub > Actions > Left side select > 003-003-002 Deploy Argo
* Right hand side > Run workflow dropdown > select options > Run Workflow

==== Destroy

* Execute the `003-003-002-destroy-argo.yml`
* Navigate to project in GitHub > Actions > Left side select > 003-003-002 Destroy Argo
* Right hand side > Run workflow dropdown > select options > Run Workflow
** Make sure the correct name and id are provided in order to remove the correct deployment

[NOTE]
The destroy workflow does not check the linting results as there is no reason for it to care. It will just go through and destroy the necessary resources.

== Known Issues

* common - node group is not created or fails to attach to the cluster
** fix - unclear at this point
* common - incorrect authentication bridging the gap between iam and rbac
** fix - review project `001-eks-std-irsa` this shows how to correctly get roles created in Terraform/OpenTofu authorized to rbac

== Reference

* https://spacelift.io/blog/github-actions-terraform
* https://docs.github.com/en/actions/how-tos/reuse-automations/reuse-workflows#passing-inputs-and-secrets-to-a-reusable-workflow
* https://spacelift.io/blog/argocd-terraform
* https://dev.to/mechcloud_academy/eks-standard-vs-eks-auto-mode-the-evolutionary-leap-in-kubernetes-operations-287g
* https://dev.to/kelechiedeh/step-by-step-guide-creating-an-amazon-eks-cluster-using-terraform-5204
* https://medium.com/@cloudcommander/argocd-on-aws-eks-as-simple-as-ever-171ae71f3a37
** look at this one for deploying to argo
* https://medium.com/@Jitendra09/from-push-to-pull-why-github-actions-argocd-is-the-devops-duo-you-need-in-2025-a38978f3dafe
* https://medium.com/@anujkishor1994/streamlining-multi-environment-argocd-management-centralized-control-for-multiple-applications-7e11a6d70c42
** this is the one to review for multi environment management
* https://7tonshark.com/posts/organize-github-workflows/
** How to organize github workflows
* https://blog.saintmalik.me/argocd-on-kubernetes-cluster/
* https://registry.terraform.io/modules/squareops/argocd/kubernetes/latest
** Module for creation
* https://dev.to/aws-builders/gitops-with-argocd-on-amazon-eks-using-terraform-a-complete-implementation-guide-1inc
** interesting as it sets argo up with a custom domain
*

== Legacy notes - not sure if useful

my local role arn

 * arn:aws:sts::792981815698:assumed-role/AWSReservedSSO_AdministratorAccess_eeb8e63974797d2b/brrAwsIdentity

* https://manski.net/articles/github/actions/cheat-sheet
* https://medium.com/@ayushishrot/how-to-automate-terraform-deployments-with-github-actions-a-step-by-step-guide-3af1ec203bb5
*

aws eks create-access-entry \
--cluster-name my-eks-cluster-example-1-xNRn9CZU \
--principal-arn arn:aws:iam::792981815698:role/aws-reserved/sso.amazonaws.com/us-east-2/AWSReservedSSO_AdministratorAccess_eeb8e63974797d2b \
--type STANDARD \
--region us-east-2


aws eks create-access-entry \
--cluster-name my-eks-cluster-example-1-f9tbDfzd \
--principal-arn arn:aws:iam::792981815698:role/aws-reserved/sso.amazonaws.com/us-east-2/AWSReservedSSO_AdministratorAccess_eeb8e63974797d2b \
--type STANDARD \
--region us-east-2

aws eks create-access-entry \
--cluster-name my-eks-cluster-example-1-f9tbDfzd \
--principal-arn arn:aws:iam::792981815698:role/github_oidc_ci_assume_role \
--type STANDARD \
--region us-east-2

// oidc role trust policy

{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"Federated": "arn:aws:iam::792981815698:oidc-provider/token.actions.githubusercontent.com"
},
"Action": "sts:AssumeRoleWithWebIdentity",
"Condition": {
"StringEquals": {
"token.actions.githubusercontent.com:aud": "sts.amazonaws.com"
},
"StringLike": {
"token.actions.githubusercontent.com:sub": "repo:benrhine/brr-base-tf-resources:ref:refs/heads/main"
}
}
}
]
}

{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"eks:*"
],
"Resource": "*"
}
]
}
